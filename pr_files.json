[{"sha": "10816cffa8044e1709763f5dc5ac5fb4d6d9ee92", "filename": ".gitignore", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/Abhi9868/AI-PR-Review-Checker/blob/12a31efa34b633163ee38e5f69c4e32a9242495d/.gitignore", "raw_url": "https://github.com/Abhi9868/AI-PR-Review-Checker/raw/12a31efa34b633163ee38e5f69c4e32a9242495d/.gitignore", "contents_url": "https://api.github.com/repos/Abhi9868/AI-PR-Review-Checker/contents/.gitignore?ref=12a31efa34b633163ee38e5f69c4e32a9242495d", "patch": "@@ -1,3 +1,5 @@\n .venv\n __pycache__\n-/__pycache__\n\\ No newline at end of file\n+/__pycache__\n+django_app/django_app/__pycache__\n+fastapi_app/fastapi_app/__pycache__\n\\ No newline at end of file"}, {"sha": "a3af0877471e44c4567193567efcc2ca01eaca7a", "filename": "sripts.py", "status": "modified", "additions": 56, "deletions": 99, "changes": 155, "blob_url": "https://github.com/Abhi9868/AI-PR-Review-Checker/blob/12a31efa34b633163ee38e5f69c4e32a9242495d/sripts.py", "raw_url": "https://github.com/Abhi9868/AI-PR-Review-Checker/raw/12a31efa34b633163ee38e5f69c4e32a9242495d/sripts.py", "contents_url": "https://api.github.com/repos/Abhi9868/AI-PR-Review-Checker/contents/sripts.py?ref=12a31efa34b633163ee38e5f69c4e32a9242495d", "patch": "@@ -1,115 +1,72 @@\n-# import requests\n+import requests\n import base64\n-# from urllib.parse import urlparse\n-from groq import Groq\n+from urllib.parse import urlparse\n \n-key='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'\n \n-def analyze_code_with_llm(file_content,file_name):\n-    prompt=f\"\"\"\n-         Aalyze the following code for :\n-         - code style and formatting issues\n-         - potential bugs and errors\n-         - performance improvements\n-         - best practices\n+def get_owner_repo(url):\n+    parsed_url = urlparse(url)\n+    if not parsed_url.path:\n+        raise ValueError(\"Invalid URL: Missing path component\")\n+    path = parsed_url.path.strip(\"/\")  # Remove leading and trailing slashes\n+    parts = path.split(\"/\")\n+    if len(parts) < 2:\n+        return None, None\n     \n-    File : {file_name}\n-    Content: {file_content}\n-    \n-    provide a detailed json output with structure\"\n-    {{\n-            \"issues\":[\n-                {{\n-                \"type\":\"<style|error|performance|best_practice>\",\n-                \"line\": <line_number>,\n-                \"description\": \"<description>\",\n-                \"suggestion\": \"<suggestion>\"\n-            }}\n-            ]\n-    }}\n-    ```json\n-    \"\"\"\n-    \n-    client = Groq(api_key=key)\n-    completion=client.chat.completions.create(\n-        model=\"llama3-70b-8192\",\n-        messages=[{\n-            \"role\":\"user\",\n-            \"content\":prompt\n-        }],\n-        temperature=1,\n-        top_p=1,\n-    )\n-    print(completion.choices[0].message.content)  \n-\n-code_str=\"ZnJvbSBmYXN0YXBpIGltcG9ydCBGYXN0QVBJLHN0YXR1cwpmcm9tIHB5ZGFu\\ndGljICBpbXBvcnQgQmFzZU1vZGVsCmZyb20gdHlwaW5nIGltcG9ydCBPcHRp\\nb25hbAoKYXBwID0gRmFzdEFQSSgpCgpjbGFzcyBBbmFseXplUFJSZXF1ZXN0\\nKEJhc2VNb2RlbCk6CiAgICByZXBvX3VybDogc3RyCiAgICBwcl9udW1iZXI6\\nIGludAogICAgZ2l0aHViX3Rva2VuOiBPcHRpb25hbFtzdHJdID0gTm9uZQoK\\nQGFwcC5wb3N0KCIvc3RhcnRfdGFzay8iKQphc3luYyBkZWYgc3RhcnRfdGFz\\na19lbmRwb2ludCh0YXNrX3JlcXVlc3Q6IEFuYWx5emVQUlJlcXVlc3QpOgog\\nICAgZGF0YT17CiAgICAgICAgInJlcG9fdXJsIjp0YXNrX3JlcXVlc3QucmVw\\nb191cmwsCiAgICAgICAgInByX251bWJlciI6dGFza19yZXF1ZXN0LnByX251\\nbWJlciwKICAgICAgICAiZ2l0aHViX3Rva2VuIjp0YXNrX3JlcXVlc3QuZ2l0\\naHViX3Rva2VuCiAgICB9CiAgICBwcmludChkYXRhKQogICAgcmV0dXJuIHsi\\ndGFza19pZCI6ICIxMjM0IiwgInN0YXR1cyI6ICJ0YXNrIHN0YXJ0ZWQifQoK\\n\" \n-\n-analyze_code_with_llm((base64.b64decode(code_str).decode(\"utf-8\")),\"script.py\")\n-\n-# def get_owner_repo(url):\n-#     parsed_url = urlparse(url)\n-#     if not parsed_url.path:\n-#         raise ValueError(\"Invalid URL: Missing path component\")\n-#     path = parsed_url.path.strip(\"/\")  # Remove leading and trailing slashes\n-#     parts = path.split(\"/\")\n-#     if len(parts) < 2:\n-#         return None, None\n-    \n-#     owner, repo = parts[0], parts[1]\n-#     return owner, repo\n+    owner, repo = parts[0], parts[1]\n+    return owner, repo\n \n \n-# def fetch_pr_files(repo_url, pr_number, github_token=None):\n-#     owner, repo = get_owner_repo(repo_url)\n-#     url=f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/files\"\n-#     headers = {\n-#         \"Authorization\": f\"token {github_token}\",\n-#     } if github_token else {}\n-#     response = requests.get(url, headers=headers)\n-#     response.raise_for_status()\n-#     return response.json()\n+def fetch_pr_files(repo_url, pr_number, github_token=None):\n+    owner, repo = get_owner_repo(repo_url)\n+    url=f\"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}/files\"\n+    headers = {\n+        \"Authorization\": f\"token {github_token}\",\n+    } if github_token else {}\n+    response = requests.get(url, headers=headers)\n+    response.raise_for_status()\n+    return response.json()\n \n-# def fetch_file_content(repo_url, file_path, github_token=None):\n-#     owner, repo = get_owner_repo(repo_url)\n-#     url=f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n-#     headers = {\n-#         \"Authorization\": f\"token {github_token}\",\n-#     } if github_token else {}\n-#     response = requests.get(url, headers=headers)\n-#     response.raise_for_status()\n-#     content= response.json()\n-#     return base64.b64decode(content[\"content\"]).decode(\"utf-8\")\n+def fetch_file_content(repo_url, file_path, github_token=None):\n+    owner, repo = get_owner_repo(repo_url)\n+    url=f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n+    headers = {\n+        \"Authorization\": f\"token {github_token}\",\n+    } if github_token else {}\n+    response = requests.get(url, headers=headers)\n+    response.raise_for_status()\n+    content= response.json()\n+    return base64.b64decode(content[\"content\"]).decode(\"utf-8\")\n     \n     \n         \n \n-# def collect_files(url, files=None):\n-#     if files is None:\n-#         files = []\n+def collect_files(url, files=None):\n+    if files is None:\n+        files = []\n \n-#     try:\n-#         response = requests.get(url)\n-#         response.raise_for_status()\n-#         data = response.json()\n+    try:\n+        response = requests.get(url)\n+        response.raise_for_status()\n+        data = response.json()\n \n-#         for item in data:\n-#             if item[\"type\"] == \"file\":\n-#                 files.append({\n-#                     \"name\": item[\"name\"],\n-#                     \"path\": item[\"path\"],\n-#                     \"download_url\": item[\"download_url\"]\n-#                 })\n-#             elif item[\"type\"] == \"dir\":\n-#                 collect_files(item[\"url\"], files)  # Recursive call\n-#     except requests.exceptions.RequestException as e:\n-#         print(f\"Error fetching {url}: {e}\")\n+        for item in data:\n+            if item[\"type\"] == \"file\":\n+                files.append({\n+                    \"name\": item[\"name\"],\n+                    \"path\": item[\"path\"],\n+                    \"download_url\": item[\"download_url\"]\n+                })\n+            elif item[\"type\"] == \"dir\":\n+                collect_files(item[\"url\"], files)  # Recursive call\n+    except requests.exceptions.RequestException as e:\n+        print(f\"Error fetching {url}: {e}\")\n     \n-#     return files\n+    return files\n \n-# if __name__ == \"__main__\":\n-#     base_url = \"https://api.github.com/repos/Abhi9868/AI-PR-Review-Checker/contents/\"\n-#     collected_files = collect_files(base_url)\n+if __name__ == \"__main__\":\n+    base_url = \"https://api.github.com/repos/Abhi9868/AI-PR-Review-Checker/contents/\"\n+    collected_files = collect_files(base_url)\n     \n-#     print(f\"Total files collected: {len(collected_files)}\")\n-#     for file in collected_files:\n-#         print(file)\n+    print(f\"Total files collected: {len(collected_files)}\")\n+    for file in collected_files:\n+        print(file)"}, {"sha": "7e4314aaa34461a190dda649ff465b28878fbca1", "filename": "zz.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Abhi9868/AI-PR-Review-Checker/blob/12a31efa34b633163ee38e5f69c4e32a9242495d/zz.py", "raw_url": "https://github.com/Abhi9868/AI-PR-Review-Checker/raw/12a31efa34b633163ee38e5f69c4e32a9242495d/zz.py", "contents_url": "https://api.github.com/repos/Abhi9868/AI-PR-Review-Checker/contents/zz.py?ref=12a31efa34b633163ee38e5f69c4e32a9242495d", "patch": "@@ -121,7 +121,6 @@ def analyze_pr(repo_url, pr_number, github_token=None):\n         pr_files = fetch_pr_files(repo_url, pr_number, github_token)\n         print(len(pr_files))\n         \n-        # Save to JSON file\n         with open(\"pr_files.json\", \"w\") as f:\n             json.dump(pr_files, f)\n         "}]